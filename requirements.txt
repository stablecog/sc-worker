diffusers==0.21.4
transformers==4.34.1
safetensors==0.4.0
ftfy==6.1.1
scipy==1.9.0
accelerate==0.24.0
bitsandbytes==0.37.2
numpy==1.24.1
opencv-python==4.7.0.68
tqdm==4.62.2
Pillow==9.4.0
timm==0.4.12
ipython==7.19.0
lingua-language-detector==1.3.2
huggingface-hub==0.15.1
boto3>=1.26.63,<2
boto3-type-annotations>=0.3.1,<1
responses==0.22.0
httpx==0.23.1
hypothesis==6.56.4
redis>=4,<5
python-dotenv
tabulate==0.9.0
pydantic==1.10.5
flask==3.0.0
waitress==2.1.2
nltk==3.8.1
pytorch-seed==0.2.0
pydub==0.25.1
moviepy==1.0.3
ffmpeg-python==0.2.0
pyloudnorm==0.1.1
PyWavelets==1.4.1
invisible-watermark==0.2.0
hf-transfer==0.1.3
pika==1.3.2
git+https://github.com/suno-ai/bark.git
git+https://github.com/ai-forever/Kandinsky-2
git+https://github.com/openai/CLIP.git
git+https://github.com/yekta/denoiser.git
https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.2/flash_attn-2.3.2+cu122torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl