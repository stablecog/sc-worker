diffusers==0.24.0
transformers==4.36.0
safetensors==0.4.1
ftfy==6.1.3
scipy==1.11.4
accelerate==0.25.0
bitsandbytes==0.41.3.post2
numpy==1.26.2
opencv-python==4.7.0.68
tqdm==4.66.1
Pillow==10.1.0
timm==0.9.12
ipython==8.18.1
lingua-language-detector==2.0.1
huggingface-hub==0.19.4
boto3>=1.26.63,<2
boto3-type-annotations>=0.3.1,<1
responses==0.24.1
httpx==0.25.2
hypothesis==6.92.0
redis>=4,<5
python-dotenv
tabulate==0.9.0
pydantic==2.5.2
flask==3.0.0
waitress==2.1.2
nltk==3.8.1
pytorch-seed==0.2.0
pydub==0.25.1
moviepy==1.0.3
ffmpeg-python==0.2.0
pyloudnorm==0.1.1
PyWavelets==1.5.0
invisible-watermark==0.2.0
hf-transfer==0.1.4
pika==1.3.2
git+https://github.com/suno-ai/bark.git
git+https://github.com/ai-forever/Kandinsky-2
git+https://github.com/openai/CLIP.git
git+https://github.com/yekta/denoiser.git
https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.2/flash_attn-2.3.2+cu122torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl